{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "christian-jamaica",
   "metadata": {},
   "source": [
    "# 03 Build The Neural Network\n",
    "\n",
    "在PyTorch中，`torch.nn`集成了大部分搭建神经网络所需要的工具，且所有的模块都继承自`nn.Module`。神经网络就是一个大的模块，里面集成了许多小的模块（layer），这样的嵌套结构能够轻松实现复杂的模型结构。\n",
    "\n",
    "以FashionMNIST为例，搭建神经网络用于分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "shaped-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Using {device} device!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "invalid-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a linear network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-industry",
   "metadata": {},
   "source": [
    "## `nn.Flatten()`\n",
    "\n",
    "`nn.Flatten()`用于将多维的张量拉伸成一维，**同时保留minibatch的维度**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incoming-jackson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2352])\n"
     ]
    }
   ],
   "source": [
    "# 以一个batch size为20的RGB图像为例\n",
    "data = torch.rand(20, 3, 28, 28)\n",
    "flatten_data = flatten_layer(data)\n",
    "print(flatten_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secret-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# 以一个batch size为3的黑白图像为例\n",
    "data = torch.rand(3, 28, 28)\n",
    "flatten_layer = nn.Flatten()\n",
    "flatten_data = flatten_layer(data)\n",
    "print(flatten_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-notion",
   "metadata": {},
   "source": [
    "## `nn.Linear`\n",
    "\n",
    "`nn.Linear`用于构建全连接层，定义时只需要给定输入和输出的维度，以batch形式进行数据输入输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "external-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flatten_data)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-desert",
   "metadata": {},
   "source": [
    "## nn.Softmax\n",
    "\n",
    "`nn.Softmax`将指定维度上的张量归一化到[0,1]之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "played-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0297, 0.0954, 0.0510, 0.0297, 0.0580, 0.0469, 0.0744, 0.0280, 0.0613,\n",
      "         0.0518, 0.0554, 0.0346, 0.0903, 0.0569, 0.0339, 0.0322, 0.0577, 0.0490,\n",
      "         0.0360, 0.0276],\n",
      "        [0.0358, 0.1267, 0.0421, 0.0293, 0.0605, 0.0307, 0.0621, 0.0307, 0.0671,\n",
      "         0.0394, 0.0665, 0.0426, 0.0669, 0.0549, 0.0470, 0.0336, 0.0461, 0.0540,\n",
      "         0.0348, 0.0293],\n",
      "        [0.0284, 0.0761, 0.0520, 0.0294, 0.0548, 0.0494, 0.0603, 0.0357, 0.0579,\n",
      "         0.0376, 0.0635, 0.0464, 0.1164, 0.0616, 0.0301, 0.0349, 0.0393, 0.0520,\n",
      "         0.0328, 0.0412]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3173, 0.3205, 0.3535, 0.3374, 0.3360, 0.3716, 0.3795, 0.2982, 0.3302,\n",
      "         0.4038, 0.3001, 0.2815, 0.3323, 0.3296, 0.3062, 0.3215, 0.4047, 0.3174,\n",
      "         0.3487, 0.2829],\n",
      "        [0.3866, 0.4296, 0.2944, 0.3358, 0.3536, 0.2458, 0.3198, 0.3300, 0.3650,\n",
      "         0.3099, 0.3637, 0.3496, 0.2487, 0.3212, 0.4280, 0.3386, 0.3263, 0.3535,\n",
      "         0.3405, 0.3041],\n",
      "        [0.2961, 0.2498, 0.3521, 0.3267, 0.3103, 0.3825, 0.3006, 0.3718, 0.3048,\n",
      "         0.2863, 0.3362, 0.3689, 0.4189, 0.3491, 0.2658, 0.3399, 0.2690, 0.3291,\n",
      "         0.3109, 0.4130]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "print(softmax(hidden1))\n",
    "softmax = nn.Softmax(dim=0)\n",
    "print(softmax(hidden1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "packed-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0226,  0.0243,  0.0088,  ..., -0.0278,  0.0331,  0.0266],\n",
      "        [-0.0170, -0.0243,  0.0055,  ..., -0.0043, -0.0339, -0.0236]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "linear.0.bias | Size: torch.Size([512]) | Values: tensor([-0.0219,  0.0115], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "linear.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[-0.0207,  0.0115,  0.0362,  ...,  0.0167, -0.0041, -0.0337],\n",
      "        [-0.0173, -0.0167, -0.0144,  ..., -0.0093,  0.0300, -0.0358]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "linear.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0012,  0.0398], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "linear.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[-0.0104,  0.0245,  0.0105,  ...,  0.0360, -0.0108, -0.0270],\n",
      "        [-0.0333,  0.0087, -0.0329,  ..., -0.0082, -0.0086, -0.0211]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "linear.4.bias | Size: torch.Size([10]) | Values: tensor([ 0.0210, -0.0323], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'{name} | Size: {param.shape} | Values: {param[:2]}' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
